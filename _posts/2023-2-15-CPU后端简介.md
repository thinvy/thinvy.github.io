---
title: CPU后端简介
author: eren
date: 2023-02-15 22:10:00 +0800
categories: [Blogging]
tags: [nn_deploy]
pin: true
math: true
mermaid: true
---

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/1.jpg" width="100%">


cpu是最为通用的nn部署后端。因为在cpu上进行编程的方式比较简单，相应实现nn推理的代码较为简单，很多不易实现的算子（尤其是部分激活函数如sigmod、tanh等）可以在cpu方便地实现进行测试。还有一些不易并行化的网络（如rnn）在cpu上部署也可以达到不错的效果。

cpu的一个重要缺点是不易实现并行化加速，对于大面积矩阵计算没有优势。不过目前已经有很多方法来解决这个问题。




## SIMD 

### 什么是SIMD指令集

SIMD (Single Instruction Multiple Data) 单指令多数据指令集，即一条指令可以处理多条数据。

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/2.jpg" width="100%">

以AVX512为例，一条指令可以一次处理16个fp32（16x32=512）或64个int8（64x8=512）等等，那么理论上使用AVX512指令集会提高16倍fp32数据的处理速度或64倍int8数据的处理速度，但一般不会达到这样的优化程度，这个原因之后会进一步探讨。

目前常见的SIMD指令集有：AVX2（256bits x86）、AVX512（512bits x86）、Neon（128bits ArmV8）等。

### 如何使用SIMD进行优化

- 使用SIMD指令集的汇编、内联汇编、C宏或API等，手动改写循环，对nn算子进行优化
- 在部分编译器的较高优化等级会对循环计算使用SIMD自动进行向量化加速（如LLVM的O2以上等级）

### SIMD优化效果的主要影响因素

实际工程中的SIMD加速并不会达到理论上的预期，我认为主要原因与Cache调度有关。

如果寄存器没有从Cache中load数据而是直接从DDR里load数据，这里的耗时会增加很多。我们可以通过观察程序运行的Cache命中率（Dismiss 次数）来判断有多少次load是直接走了DDR。

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/3.jpg" width="100%">

**使用SIMD可能会大幅降低Cache命中率**

如果程序没有被优化而是按照循环中一次次读入单个数据进行计算，这时的Cache命中率会较高。如果程序中手动加入了向量化计算的指令但并没有对内存调度进行优化，虽然执行速度明显提升，但Cache命中率却降低了。

可以这样理解，在不对内存调度进行优化时，因为向量化后的一次load的数据变多，load之后需要从DDR中转运到Cache中的数据也会变多，可能还没有来得及将DDR中的数据转入Cache中，下一次load就已经来了，那这次load只能从DDR中直接进行了。

### SIMD小结

SIMD优化主要是在硬件资源确定的情况下，在软件层面针对计算的优化，并且当前很多的编译器已经针对常用的大规模数据操作进行了SIMD优化，对于开发者十分友好。SIMD指令集对于cpu后端的部署优化提供了更大的灵活性以及cpu资源更充分的利用。

一般来说，直接编写人为优化的汇编或者编译器高等级优化，在Cache命中率上的表现较好，在C代码中加入向量化计算指令（不做编译器高级优化）在Cache命中率的表现上较差。所以较为简单的矩阵计算可以考虑直接编写C循环使用编译器进行优化，但较为复杂的Conv等算子可能还是需要手动优化了。


## Multi-Core Processor

在2005年左右，处理器的主频已经接近4G，这时再追求更高的主频的成本很高但对计算性能提升的效果并不明显，大家逐渐开始探索多核技术。下图是多核处理器的主要结构。

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/4.jpg" width="100%">

多核处理器内部主要由处理单元、内存控制单元以及PCIe单元三种IP构成，三者通过片内高速总线连接。每个处理单元内都有一个私有的Cache，这些Cache会通过某些机制和与内存管理单元连接的DDR进行同步。

多核处理器通过并行地进行数据处理，大大提升了CPU后端的计算性能和多任务处理的能力。

## New Trends 

随着芯片制造工艺和封装工艺的不断进步，CPU后端设计有了部分新的发展趋势。以下是我个人对这些趋势的了解。

### Intel

Intel Core 11th 处理器基本上全部加入了AVX-512指令集，这对于NN推理等业务的性能提升很大。我在TVM上开启AVX-512相比开启AVX2有推理yolov5近两倍的推理速度提升。但是 Intel Core 12th 之后AVX-512被砍掉了，取而代之的是“大小核”下的超级多个核心带来的并行度。当然在这种CPU的设计中再加入AVX-512的话发热量一定会很高，所以被禁用是正常的。可能 Intel 认为多核并行相较于大SIMD指令集带来的综合性能提升更为明显。

### RISC-V

RISC-V 架构是基于 精简指令集计算（RISC）原理建立的开放 指令集架构（ISA），其完全开源，设计简单，易于移植Unix系统，并且使用模块化设计，拥有完整工具链，同时有大量的开源实现和流片案例，得到很多芯片公司的认可。

下面提到的这个RISC-V芯片比较令我震惊，它严格来说属于AI加速器而非处理器。

Hot Chips 29 大会上，基于RISC-V核心的AI芯片Celerity一亮相便引起开源社区的关注。Celerity多级结构组成。它们分别为通用级（General-Purpose Tier），众核级（Manycore Tier）和专用级（ Specialization Tier），三级之间两两互连。

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/5.png" width="100%">

通用级具备完整的计算功能，可以执行各类计算操作以及与内存、I/O和板载芯片的通信。通用级也可用于承载操作系统。通用级的功能多样，能效较低，可运行在625MHz。

众核级由496个低功耗 RISC-V Vanilla -5 标量处理核心阵列（16x31）组成，负责粗粒度与细粒度的并行计算。这些 Vanilla -5 处理核心由 80Gbps 的全双工片上网络（NoC）连接在一起。众核级的功能与能效相对折中。

专用级则由专门用于AI计算的二值神经网络（BNN）核心组成。该BNN核心可直接支持13.4M大小的9层模型（包括一层定点卷积层，6层二值卷积层与2层全连接层）。专用级功能单一，却具有最高的能效。

Celerity的设计兼顾了部分处理器的通用性和AI加速器的高效性，采用的模块化IP设计也是芯片设计十分值得学习的优点。对于这一类芯片可能今后还需要更多地学习和验证它相较于一般的CPU、DSP、NPU等后端的优缺点。

### Apple

Apple在2020年发布M1系列芯片和相应的产品，通过使用Arm替换x86的核心在维持高性能的同时极大程度降低了功耗，同时带来了一个十分重要的创新，那就是Apple通过其及其先进的封装技术将CPU、GPU、VPU、NPU、RAM等封装在了一个芯片内。这个创新大大提高了内存访问的带宽，同时简化了外围硬件设计。

<img decoding="async" src="https://raw.githubusercontent.com/erennere/erennere.github.io/gh-pages/_imgs/2023-2-15-CPU%E5%90%8E%E7%AB%AF%E7%AE%80%E4%BB%8B/6.jpg" width="100%">

这种封装技术或许可以使软硬件的开发都更为模块化，也即针对每一个单元单独优化算法和部署，根据不同的业务将这些单元封装成为一个整体。这样相对于直接针对业务设计芯片的流片在成本上和开发时间上都会有较大的提升。



**参考资料：**

https://zhuanlan.zhihu.com/p/461704887

https://zhuanlan.zhihu.com/p/115114220

https://ieeexplore.ieee.org/document/8344478


